{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedesNeurais05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpelogia/Optimization-and-Neural-Networks/blob/master/RedesNeurais05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7evwO02fkTQw",
        "colab_type": "text"
      },
      "source": [
        "#Redes Neurais\n",
        "[Voltar](https://colab.research.google.com/drive/1zGxVatpjlZtdtECAikT1eKP7EQ50m-Ur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scxNg3i1kZ1S",
        "colab_type": "text"
      },
      "source": [
        "#5 - Método do Gradiente\n",
        "\n",
        "É um dos métodos mais clássicos no estudo de otimização. O método é iterativo e consiste em avançar, a partir de um ponto inicial, na direção oposta à do gradiente da função neste ponto, isto é, na direção de maior decrescimento. O tamanho do passo deve ser definido de modo que o ponto $x^{k+1}$ esteja mais próximo da solução (do mínimo global) do que $x^k$.\n",
        "\n",
        "Basicamente:\n",
        "\n",
        "$$\\large x^{k+1} = x^k - t^k \\nabla f(x^k)$$\n",
        "\n",
        "\n",
        "![Image](https://drive.google.com/uc?id=1bEbFgkj1FNzb11nlx3uNMzpT1Q0dC5Wi)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rioia6qmkdsX",
        "colab_type": "code",
        "outputId": "33d53eaf-7f5d-4286-f9fd-4d266bdfdd94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as la \n",
        "\n",
        "def func(x):\n",
        "    return x[0]**2 +2.0*x[1]**2\n",
        "def grad_func(x):\n",
        "    return [ 2.0*x[0], 4.0*x[1]]\n",
        "\n",
        "def metodo_gradiente(f, grad, x, t):\n",
        "    # ||grad|| é o ERRO\n",
        "    n_iter = 0\n",
        "    while (np.log10(la.norm(grad(x))) > -6.0):\n",
        "        x = x - np.multiply(t,grad(x))\n",
        "        n_iter = n_iter + 1\n",
        "    \n",
        "    print('x',n_iter,' =', x)\n",
        "    print('f = ',func(x))\n",
        "\n",
        "\n",
        "metodo_gradiente(func,grad_func,[1,2],0.495)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x 787  = [ 0.00000000e+00 -2.48857977e-07]\n",
            "f =  1.2386058502993438e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci6YvcF_iytZ",
        "colab_type": "text"
      },
      "source": [
        "##5.1 -Método do Gradiente e Backpropagation\n",
        "\n",
        "Seja uma rede neural de $p$ camadas com saída $o_i = f_p(w_pf_{p-1}(w_{p-1}f_{p-2}(\\cdots))$ .\n",
        "\n",
        "Sendo $y_i$ a saída esperada, podemos definir uma função $A(x,w)$ que representa o erro dessa saída da seguinte forma:\n",
        "$$A(x,w) = \\sum_{i=1}^{N}o_i - y_i$$\n",
        "\n",
        "$$\\implies A(x,w) = \\sum_{i =1}^{N}f_p(w_pf_{p-1}(w_{p-1}f_{p-2}(\\cdots)) - y_i)^2 $$\n",
        "\n",
        "Pensando em aplicar o método do gradiente para otimizar cada peso $w$ da rede, temos algo do tipo:\n",
        "$$w^{k+1} = w^{k} - t \\nabla A(x,w)$$\n",
        "\n",
        "Mas para calcular $\\nabla A(x,w)$ precisamos utilizar a regra da cadeia para várias variáveis, tendo em vista que $A(x,w)$ é uma composição de funções.\n",
        "\n",
        "Mais especificamente, para os pesos de cada camada:\n",
        "\n",
        "$$\\text{camada }p: w_p^{k+1} = w_p^{k} - t \\frac{\\partial A}{\\partial w_p} $$\n",
        "\n",
        "$$\\text{camada }p-1:w_{p-1}^{k+1} = w_{p-1}^{k} - t \\frac{\\partial A}{\\partial w_{p-1}} $$\n",
        "$$\\cdots$$\n",
        " \n",
        "Onde, para a camada p:\n",
        "$$\\frac{\\partial A(x,w)}{\\partial w_p} = \\sum_{i=1}^{N}2(f_p(w_p f_{p-1}(w_{p-1}f_{p-2}(\\cdots)))-y_i)\\cdot f_{p}^{'}(w_p f_{p-1}(\\cdots))\\cdot \\color{red}{f_{p-1}(\\cdots)}$$\n",
        "e para a camada p-1:\n",
        "$$\\frac{\\partial A(x,w)}{\\partial w_p} = \\sum_{i=1}^{N}2R_i\\cdot f_{p}^{'}(w_p f_{p-1}(\\cdots)) \\cdot w_p \\cdot f_{p-1}^{'}(\\cdots)\\cdot\\color{red}{f_{p-2}(\\cdots)}$$\n",
        "$$\\small \\text{obs:  } R_i = o_i - y_i$$\n",
        "\n",
        "\n",
        "Perceba que para calcular $\\frac{\\partial A(x,w)}{\\partial w_k}$, utilizamos a saída da camada $k-1$.\n",
        "\n",
        "\n",
        "Logo, uma maneira mais abstrata de se representar os passos da otimização dos pesos é:\n",
        "$$w =  w + \\Delta w$$\n",
        "$$\\text{onde } \\Delta w = -2\\cdot R_i \\cdot f^{'}(wp\\cdot o_{i,p-1})\\cdot o_{i,p-1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsoLq-UUbLJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
