{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedesNeurais.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpelogia/Optimization-and-Neural-Networks/blob/master/RedesNeurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B3b4KZX7stq",
        "colab_type": "text"
      },
      "source": [
        "#Estudo de Redes Neurais\n",
        "Aluno : Frederico José Ribeiro Pelogia\n",
        "\n",
        "Orientador : Luis Felipe Cesar da Rocha Bueno\n",
        "\n",
        "#Índice\n",
        "1. [Função de ativação linear](https://colab.research.google.com/drive/1J5eLQuRxyFlVI_aRJ2-H_oYzRW_fRPVG)\n",
        "    \n",
        "2. [Função de ativação Sigmoide](https://colab.research.google.com/drive/1q7eV2O318LeqEvQZZRTfHYn3ErJ7nky4)\n",
        "\n",
        "3. [Função de ativação $max(x,0)$](https://colab.research.google.com/drive/1s8oc1PofMhJAtYPFfffpTbCKcgehjmhW)\n",
        "    \n",
        "4. [Função de ativação $\\frac{x + \\sqrt{x^2 +0.0001} }{2}$](https://colab.research.google.com/drive/1c6m2lElZbDFNKq8f-ISMEQguB-cUDlp1)\n",
        "    \n",
        " \n",
        " \n",
        "5. [Método do Gradiente e Backpropagation](https://colab.research.google.com/drive/1XhmG4nNIG6wWobuuJN6l2yzx3y0ZmZXX)\n",
        "\n",
        "6. [Utilizando Viés (Bias)](https://colab.research.google.com/drive/1xTZi2Bnf_4sdX_vqWSy3ArxHgj343IeK)\n",
        "\n",
        "7. [Aumentando o número de camadas ocultas](https://colab.research.google.com/drive/1IyabWmbgWbK02Y1j-d33ggjG8VqGUcbm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqGtAL9YxkUG",
        "colab_type": "text"
      },
      "source": [
        "#APENAS TESTES DAQUI PRA BAIXO, IGNORAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXi-kvMjxit5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epsilon = 0.0001\n",
        "\n",
        "def reLu(x):\n",
        "    return 0.5*(x + np.sqrt(x**2 + epsilon))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-1*x)) \n",
        "\n",
        "class Layer():\n",
        "    def __init__(self, n_values, func = \"none\"):\n",
        "        self.n_values = n_values\n",
        "        self.func = func\n",
        "        self.arr = np.random.randn(n_values)\n",
        "        self.biases = np.random.randn(n_values)\n",
        "\n",
        "    \n",
        "class NeuralNetwork():\n",
        "  \n",
        "    def __init__(self, layers, learning_rate = 0.0005 ,optimizer = \"backpropagation\"):\n",
        "        self.layers = layers\n",
        "        self.n_layers = len(layers)\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        #inicializa as matrizes com os pesos\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            n_curr = self.layers[i].n_values\n",
        "            n_next = self.layers[i+1].n_values\n",
        "            self.layers[i].weights = np.random.randn(n_curr,n_next)\n",
        "\n",
        "    def train_step(self, input, target):\n",
        "\n",
        "        self.layers[0].arr = input\n",
        "\n",
        "\n",
        "        # Feed-Forward\n",
        "        for i in range(self.n_layers):\n",
        "            \n",
        "            if i!= 0:\n",
        "                self.layers[i].arr = np.dot(self.layers[i-1].arr, self.layers[i-1].weights)        \n",
        "                self.layers[i].arr = np.add(self.layers[i].arr, self.layers[i].biases)\n",
        "                \n",
        "                if self.layers[i].func == \"sigmoid\":\n",
        "                    for j in range(self.layers[i].n_values):\n",
        "                        self.layers[i].arr[0][j] = sigmoid(self.layers[i].arr[0][j]) \n",
        "                elif self.layers[i].func == \"relu\":\n",
        "                    for j in range(self.layers[i].n_values):\n",
        "                        self.layers[i].arr[0][j] = reLu(self.layers[i].arr[0][j]) \n",
        "                else:\n",
        "                    if not self.layers[i].func == \"linear\":\n",
        "                        print(\"ERROR: unknown activation function {}\".format(self.layers[i].func))\n",
        "\n",
        "        output = self.layers[self.n_layers-1].arr\n",
        "\n",
        "        # Otimização\n",
        "   \n",
        "        if self.optimizer == \"backpropagation\":\n",
        "\n",
        "            for i in range(self.n_layers-1,0,-1):\n",
        "                if i == self.n_layers-1:\n",
        "                    error = target - output\n",
        "                else:\n",
        "                    error = np.dot(error, np.transpose(self.layers[i].weights))\n",
        "\n",
        "                if self.layers[i].func == \"linear\":\n",
        "                    d_func = 1\n",
        "                elif self.layers[i].func == \"sigmoid\":\n",
        "                    d_func = self.layers[i].arr*(1 - self.layers[i].arr)\n",
        "                elif self.layers[i].func == \"relu\":\n",
        "                    d_func = 0.5*(self.layers[i].arr/np.sqrt(self.layers[i].arr**2 + epsilon) + 1)\n",
        "                else:\n",
        "                    print(\"ERROR: unknown activation function {}\".format(self.layers[self.n_layers - 1].func))\n",
        "\n",
        "                gradient = self.learning_rate * error * d_func\n",
        "                \n",
        "                self.layers[i].biases = np.add(self.layers[i].biases, gradient)\n",
        "\n",
        "                weights_deltas = np.dot(np.transpose(self.layers[i-1].arr), gradient)\n",
        "                self.layers[i-1].weights = np.add(self.layers[i-1].weights, weights_deltas)\n",
        "\n",
        "        # IMPLEMENTAR PARA MÉTODO DO GRADIENTE TBM\n",
        "        elif self.optimizer == \"gradient\":\n",
        "          \n",
        "\n",
        "    def predict(self, input):\n",
        "\n",
        "        self.layers[0].arr = input\n",
        "\n",
        "        # Feed-Forward\n",
        "        for i in range(len(self.layers)):\n",
        "            if i!= 0:\n",
        "                self.layers[i].arr = np.dot(self.layers[i-1].arr, self.layers[i-1].weights)        \n",
        "                self.layers[i].arr = np.add(self.layers[i].arr, self.layers[i].biases)\n",
        "                \n",
        "                if self.layers[i].func == \"sigmoid\":\n",
        "                    for j in range(self.layers[i].n_values):\n",
        "                        self.layers[i].arr[0][j] = sigmoid(self.layers[i].arr[0][j]) \n",
        "                elif self.layers[i].func == \"relu\":\n",
        "                    for j in range(self.layers[i].n_values):\n",
        "                        self.layers[i].arr[0][j] = reLu(self.layers[i].arr[0][j]) \n",
        "                else:\n",
        "                    if not self.layers[i].func == \"linear\":\n",
        "                        print(\"ERROR: unknown activation function {}\".format(self.layers[i].func))\n",
        "\n",
        "        output = self.layers[self.n_layers-1].arr\n",
        "        return output\n",
        "    \n",
        "    \n",
        "    def train(self, trainning_set, labels, epochs=1000):\n",
        "        for _ in range(epochs):\n",
        "            index  = np.random.randint(len(trainning_set))\n",
        "            self.train_step(trainning_set[index], labels[index])        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "layers = [Layer(1), Layer(2,\"relu\"), Layer(2,\"relu\"),Layer(1,\"linear\")]\n",
        "\n",
        "X = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "y = X**2\n",
        "\n",
        "mynn = NeuralNetwork(layers, 0.00005, optimizer= \"backpropagation\")\n",
        "mynn.train(X, y, 10000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jpS-_Maxw9u",
        "colab_type": "code",
        "outputId": "b90a262c-91ce-4d01-d8ba-16dc7589a182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "w1 = layers[0].weights[0][0]\n",
        "w2 = layers[0].weights[0][1]\n",
        "\n",
        "b1 = layers[1].biases[0][0]\n",
        "b2 = layers[1].biases[0][1]\n",
        "\n",
        "w3 = layers[1].weights[0][0]\n",
        "w5 = layers[1].weights[1][0]\n",
        "\n",
        "w4 = layers[1].weights[0][1]\n",
        "w6 = layers[1].weights[1][1]\n",
        "\n",
        "b3 = layers[2].biases[0][0]\n",
        "b4 = layers[2].biases[0][1]\n",
        "\n",
        "w7 = layers[2].weights[0][0]\n",
        "w8 = layers[2].weights[1][0]\n",
        "b5 = layers[3].biases[0][0]\n",
        "\n",
        "\n",
        "\n",
        "#(w1w3w7 + w2w5w7 + w2w6w8 + w1w4w8)x \n",
        "#  + (w3w7b1 + w5w7b2 + w7b3 + w6w8b2 + w4w8b1 + w8b4 + b5)\n",
        "\n",
        "\n",
        "A = -1*b1/w1\n",
        "B = -1*b2/w2\n",
        "C = -1*(w3*b1 + w5*b2 + b3)/(w3*w1 + w5*w2)\n",
        "D = -1*(w6*b2 + w4*b1 + b4)/(w6*w2 + w4*w1)\n",
        "\n",
        "\n",
        "ex = np.linspace(0,60,60)\n",
        "ey = np.array(ex) \n",
        "\n",
        "\n",
        "for i in range(len(ey)):\n",
        "  \n",
        "  if (ey[i] < A and ey[i] >= B and ey[i] >= C and ey[i] >=D ):\n",
        "    ey[i] = (w5*w2*w7 + w8*w2*w6)*ey[i] + b5\n",
        "  \n",
        "  elif (ey[i] < B and ey[i]>=A and ey[i]>=C and ey[i]>=D):\n",
        "    ey[i] = (w7*w3*w1 + w8*w4*w1)*ey[i] + b5\n",
        "  \n",
        "  elif(ey[i] < A and ey[i] <B and ey[i]>=C and ey[i]>=D):\n",
        "    ey[i] = b3*w7+b4*w8+b5\n",
        "  \n",
        "  elif(ey[i]<C and ey[i] <D and ey[i]>=A and ey[i]>=B):\n",
        "    ey[i] = b5\n",
        "  \n",
        "  elif(ey[i]<A and ey[i]<B and ey[i]<C and ey[i]>=D):\n",
        "    ey[i] = b4*w8+b5\n",
        "    \n",
        "  elif(ey[i]<A and ey[i]<B and ey[i]< D and ey[i]>=C):\n",
        "    ey[i] = b3*w7+b5\n",
        "\n",
        "  elif(ey[i]<A and ey[i]<C and ey[i]<D and ey[i]>=B):\n",
        "    ey[i] = b5\n",
        "  \n",
        "  elif(ey[i]<B and ey[i]<C and ey[i]<D and ey[i]>=A):\n",
        "    ey[i] = b5\n",
        "  \n",
        "  elif(ey[i]<C and ey[i]>=A and ey[i]>=B and ey[i]>=D):\n",
        "    ey[i] = ((w2*ey[i] + b2)*w6 + (w1*ey[i] + b1)*w4 + b4)*w8 + b5\n",
        "  \n",
        "  elif(ey[i]<D and ey[i]>=A and ey[i]>=B and ey[i]>=C):\n",
        "    ey[i] = ((w1*ey[i] + b1)*w3 + (w2*ey[i] + b2)*w5 + b3)*w7 + b5\n",
        "    \n",
        "  elif(ey[i]>=A and ey[i]>=B and ey[i]>= C and ey[i]>=D):\n",
        "    ey[i] = (w1*w3*w7 + w2*w5*w7 + w2*w6*w8 + w1*w4*w8)*ey[i] + (w3*w7*b1 + w5*w7*b2 + w7*b3 + w6*w8*b2 + w4*w8*b1 + w8*b4 + b5)\n",
        " \n",
        "\n",
        "    \n",
        "plt.axis([0,20,0,20])\n",
        "\n",
        "\n",
        "plt.scatter(X,y,s = 30, c = \"red\")\n",
        "plt.plot(ex,ey)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd8a6b35e10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BekUbZUDp_8x",
        "colab_type": "code",
        "outputId": "7ecd6b13-3764-4835-ed0a-516b6e84d9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}