{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as la \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "func = lambda x: max(x,0.0)\n",
    "\n",
    "func = np.vectorize(func)\n",
    "\n",
    "def d_func(x):\n",
    "    dx = x\n",
    "    dx[x<0.0] = 0\n",
    "    dx[x>=0.0] = 1\n",
    "    return dx\n",
    "\n",
    "\n",
    "\n",
    "def backpropagation(I, y, w1, w2):\n",
    "    H_unac = np.dot(w1,I) \n",
    "    H = func(H_unac)\n",
    "    O_unac = np.dot(w2,H)\n",
    "    O = func(O_unac)\n",
    "\n",
    "    error = O - y\n",
    "    alpha = error*d_func(O_unac)\n",
    "    \n",
    "    \n",
    "    grad = np.transpose(np.multiply(alpha , H)) \n",
    "    \n",
    "    erro_h = np.dot(np.transpose(w2), alpha)\n",
    "    \n",
    "    alpha_h = np.multiply(erro_h, d_func(H_unac))\n",
    "    \n",
    "    grad_h = np.multiply(alpha_h, np.transpose(np.tile(I, (1, 2))))\n",
    "\n",
    "    return grad, grad_h\n",
    "\n",
    "\n",
    "def feedforward(I,w1,w2):\n",
    "    H_unac = np.dot(w1,I) # unac indica que é a versao não ativada da camada\n",
    "    H = func(H_unac)\n",
    "    O_unac = np.dot(w2,H)\n",
    "    O = func(O_unac)\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#erro quadrático (funcao a ser minimizada)\n",
    "def eq(x,y,w1,w2):\n",
    "    parte_1 = (feedforward(x[0],w1,w2) - y[0])**2\n",
    "    parte_2 = (feedforward(x[1],w1,w2) - y[1])**2\n",
    "    parte_3 = (feedforward(x[2],w1,w2) - y[2])**2\n",
    "    parte_4 = (feedforward(x[3],w1,w2) - y[3])**2\n",
    "    parte_5 = (feedforward(x[4],w1,w2) - y[4])**2\n",
    "\n",
    "    return parte_1 + parte_2 + parte_3 + parte_4 + parte_5\n",
    "\n",
    "def calcula_G(g_hist, k):\n",
    "    quad_sum = 0\n",
    "    for i in range(k):\n",
    "        quad_sum = quad_sum + quad(g_hist[i])\n",
    "    return quad_sum\n",
    "\n",
    "def calcula_G_h(gh_hist, k):\n",
    "    quad_sum = 0\n",
    "    for i in range(k):\n",
    "        quad_sum = quad_sum + quad(gh_hist[i])\n",
    "    return quad_sum\n",
    "\n",
    "\n",
    "#funcao que calcula o gradiente da funcao de erro quadratico\n",
    "def grad_f(x,y,w1,w2,batch_size):\n",
    "  temp_grad = 0\n",
    "  temp_grad_h = 0\n",
    "  indices = random.sample(range(0, len(x)), batch_size)\n",
    "  for i in range(batch_size):\n",
    "    temp_grad, temp_grad_h =  backpropagation(x[indices[i]],y[indices[i]], w1, w2)\n",
    "    if(i==0):\n",
    "      grad = temp_grad\n",
    "      grad_h = temp_grad_h\n",
    "    else:\n",
    "      grad = grad + temp_grad\n",
    "      grad_h = grad_h + temp_grad_h\n",
    "  grad = grad/batch_size\n",
    "  grad_h = grad_h/batch_size\n",
    "  return grad, grad_h\n",
    "\n",
    "\n",
    "\n",
    "gh_hist = []\n",
    "g_hist = []\n",
    "\n",
    "def Adagrad(X, y, ni, epsilon, batch_size = 5):\n",
    "\n",
    "    # w1 e w2 são matrizes com os pesos da rede neural\n",
    "    w1 = np.matrix([[1],[2]])\n",
    "    w2 = np.matrix([3,4])\n",
    "\n",
    "    n_iter = 0\n",
    "    erro_quad = 0\n",
    "    a_history = []\n",
    "    b_history = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        #descobre o gradiente\n",
    "        grad, grad_h = grad_f(X, y, w1, w2, batch_size)\n",
    "\n",
    "\n",
    "        #=========== apenas para plotar o erro==========\n",
    "        a = n_iter\n",
    "        a_history.append(a)\n",
    "        erro_quad = eq(X,y,w1,w2)\n",
    "        b = erro_quad \n",
    "        b_history.append(b)\n",
    "        #===============================================\n",
    "        \n",
    "        g_hist.append(grad)\n",
    "        gh_hist.append(grad_h)\n",
    "        \n",
    "        G = calcula_G(g_hist, n_iter)\n",
    "        G_h = calcula_G_h(gh_hist, n_iter)\n",
    "        #anda com passo t na direção oposta à do gradiente\n",
    "        w1 = w1 - np.multiply(ni/(np.sqrt(G_h) + epsilon), grad_h)\n",
    "        w2 = w2 - np.multiply(ni/(np.sqrt(G) + epsilon), grad)        \n",
    "        \n",
    "        #norma do gradiente (usado como medida de errosi)\n",
    "        norm_grad = la.norm(np.matrix([[grad_h[0,0]],[grad_h[1,0]],[grad[0,0]],[grad[0,1]]]))\n",
    "\n",
    "        \n",
    "        #condição de parada\n",
    "        if(norm_grad < 0.0001):\n",
    "          break\n",
    "\n",
    "        n_iter = n_iter + 1\n",
    "\n",
    "\n",
    "#=============== plotando resultados ====================\n",
    "\n",
    "    # m ideal por quadrados mínimos : 4.09\n",
    "    m = (w1[0,0]*w2[0,0] + w1[1,0]*w2[0,1])\n",
    "    print('m: ',  m)\n",
    "    print('n_iter', n_iter)\n",
    "\n",
    "    ex = np.linspace(0,6,10)\n",
    "    ey = m*ex\n",
    "\n",
    "    plt.axis([0,6,0,30])\n",
    "    plt.scatter(X,[1,4,9,16,25],s = 30, c = \"red\")\n",
    "\n",
    "    plt.plot(ex,ey)\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(a_history,b_history)\n",
    "    plt.show()\n",
    "#========================================================\n",
    "\n",
    "\n",
    "#aqui temos os dados para treinamento\n",
    "\n",
    "X = [np.transpose(np.matrix([1])),\n",
    "     np.transpose(np.matrix([2])),\n",
    "     np.transpose(np.matrix([3])),\n",
    "     np.transpose(np.matrix([4])),\n",
    "     np.transpose(np.matrix([5]))\n",
    "     ]\n",
    "\n",
    "y = [1,4,9,16,25]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Adagrad(X, y,0.01 ,1e-8 , 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[228.]\n",
      " [304.]]\n",
      "\n",
      "b: [[-35.41914214]\n",
      " [-44.56295629]]\n",
      "\n",
      "\n",
      "[[53238.51563022]\n",
      " [94401.85707313]]\n"
     ]
    }
   ],
   "source": [
    "a = gh_hist[0]\n",
    "b = gh_hist[1]\n",
    "print('a:',a)\n",
    "print()\n",
    "print('b:',b)\n",
    "print()\n",
    "print()\n",
    "quad = lambda x:x**2\n",
    "quad = np.vectorize(quad)\n",
    "print(quad(a) + quad(b))\n",
    "\n",
    "\n",
    "\n",
    "def calcula_G_h(gh_hist, k):\n",
    "    quad_sum = 0\n",
    "    for i in range(k):\n",
    "        quad_sum = quad_sum + quad(gh_hist[i])\n",
    "    return quad_sum\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[53263.47083437],\n",
       "        [94442.18886136]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcula_G(gh_hist, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[ 76. 152.]]\n",
      "\n",
      "b: [[ -1.15611443 -10.29992858]]\n",
      "\n",
      "\n",
      "[[ 5777.33660058 23210.08852867]]\n"
     ]
    }
   ],
   "source": [
    "a = g_hist[0]\n",
    "b = g_hist[1]\n",
    "print('a:',a)\n",
    "print()\n",
    "print('b:',b)\n",
    "print()\n",
    "print()\n",
    "quad = lambda x:x**2\n",
    "quad = np.vectorize(f)\n",
    "print(quad(a) + quad(b))\n",
    "\n",
    "\n",
    "def calcula_G(g_hist, k):\n",
    "    quad_sum = 0\n",
    "    for i in range(k):\n",
    "        quad_sum = quad_sum + quad(g_hist[i])\n",
    "    return quad_sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5777.51756286, 23213.25946767]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcula_G(g_hist, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.15611443 -10.29992858]]\n"
     ]
    }
   ],
   "source": [
    "x = g_hist[1]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.33660058, 106.08852867]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(x,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
